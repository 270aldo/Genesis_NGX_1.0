name: Beta Validation Tests

on:
  push:
    branches: [ main, develop, beta ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_category:
        description: 'Test category to run (leave empty for all)'
        required: false
        type: choice
        options:
          - ''
          - 'user_frustration'
          - 'edge_cases'
          - 'multi_agent'
          - 'ecosystem_integration'
          - 'stress_tests'

jobs:
  quick-validation:
    name: Quick Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        cd backend
        poetry install --with dev
    
    - name: Run quick validation
      run: |
        cd backend
        CI_STAGE=quick ./scripts/beta_validation_ci.sh
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quick-validation-results
        path: backend/artifacts/

  full-validation:
    name: Full Beta Validation
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false
    
    strategy:
      matrix:
        category: 
          - user_frustration
          - edge_cases
          - multi_agent
          - ecosystem_integration
          - stress_tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        cd backend
        poetry install --with dev
    
    - name: Run ${{ matrix.category }} tests
      run: |
        cd backend
        poetry run python -m tests.beta_validation.run_beta_validation \
          --category ${{ matrix.category }} \
          --output-dir reports
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: ${{ matrix.category }}-results
        path: backend/reports/

  consolidate-results:
    name: Consolidate Results
    needs: full-validation
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-results/
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Consolidate reports
      run: |
        python3 << 'EOF'
        import json
        import glob
        from pathlib import Path
        
        # Find all result files
        result_files = glob.glob('all-results/**/beta_validation_results_*.json', recursive=True)
        
        if not result_files:
            print("No result files found")
            exit(1)
        
        # Aggregate results
        total_passed = 0
        total_failed = 0
        total_scenarios = 0
        all_critical_issues = []
        
        for file in result_files:
            with open(file, 'r') as f:
                data = json.load(f)
                summary = data.get('summary', {})
                total_passed += summary.get('passed', 0)
                total_failed += summary.get('failed', 0)
                total_scenarios += summary.get('total_scenarios', 0)
                all_critical_issues.extend(data.get('critical_issues', []))
        
        # Calculate overall pass rate
        pass_rate = (total_passed / total_scenarios * 100) if total_scenarios > 0 else 0
        
        print(f"Overall Results:")
        print(f"- Total Scenarios: {total_scenarios}")
        print(f"- Passed: {total_passed}")
        print(f"- Failed: {total_failed}")
        print(f"- Pass Rate: {pass_rate:.1f}%")
        print(f"- Critical Issues: {len(all_critical_issues)}")
        
        # Create summary for PR comment
        with open('summary.md', 'w') as f:
            f.write("## ü§ñ Beta Validation Results\n\n")
            f.write(f"**Pass Rate:** {pass_rate:.1f}%\n\n")
            f.write(f"| Metric | Value |\n")
            f.write(f"|--------|-------|\n")
            f.write(f"| Total Scenarios | {total_scenarios} |\n")
            f.write(f"| Passed | ‚úÖ {total_passed} |\n")
            f.write(f"| Failed | ‚ùå {total_failed} |\n")
            
            if all_critical_issues:
                f.write(f"\n### ‚ö†Ô∏è Critical Issues Found\n\n")
                for issue in all_critical_issues[:5]:
                    f.write(f"- **{issue['category']}**: {issue['scenario']}\n")
                
                if len(all_critical_issues) > 5:
                    f.write(f"\n_...and {len(all_critical_issues) - 5} more_\n")
            
            if pass_rate >= 90 and not all_critical_issues:
                f.write(f"\n### ‚úÖ Ready for BETA Launch\n")
            elif pass_rate >= 70:
                f.write(f"\n### ‚ö†Ô∏è Improvements Needed\n")
            else:
                f.write(f"\n### ‚ùå Not Ready for BETA\n")
        
        # Set exit code
        if all_critical_issues:
            exit(2)
        elif pass_rate < 90:
            exit(1)
        else:
            exit(0)
        EOF
    
    - name: Comment PR
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });
    
    - name: Update status badge
      if: github.ref == 'refs/heads/main'
      run: |
        # This would update a badge in README or external service
        echo "Badge update would happen here"

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.test_category == 'stress_tests')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        cd backend
        poetry install --with dev
    
    - name: Run performance tests
      run: |
        cd backend
        CI_STAGE=performance ./scripts/beta_validation_ci.sh
    
    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'customBiggerIsBetter'
        output-file-path: backend/reports/benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true